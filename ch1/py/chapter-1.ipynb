{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ffa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = model.invoke(\"The sky is\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89681673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0db50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"\"\"The most recent advancements in NLP are being driven by Large Language Models (LLMs). \n",
    "         These models outperform their smaller counterparts and have become invaluable for developers \n",
    "         who are creating applications with NLP capabilities. \n",
    "         Developers can tap into these models through Hugging Face's `transformers` library,\n",
    "         or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\"\"\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''Answer the question based on the context below. If the \n",
    "            question cannot be answered using the information provided, answer \n",
    "            with \"I don\\'t know\".''',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": '''The most recent advancements in NLP are being driven by \n",
    "        Large Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, \n",
    "        respectively.''',\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(model.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "response = structured_llm.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58462b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "completion = model.invoke(\"Hi there!\")\n",
    "# Hi!\n",
    "\n",
    "completions = model.batch([\"Hi there!\", \"Bye!\"])\n",
    "# ['Hi!', 'See you!']\n",
    "\n",
    "for token in model.stream(\"Bye!\"):\n",
    "    print(token)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9448e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# the building blocks\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# combine them in a function\n",
    "# @chain decorator adds the same Runnable interface for any function you write\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "\n",
    "# use it\n",
    "\n",
    "response = chatbot.invoke({\"question\": \"In GenAI, Which model providers offer LLMs?\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "\n",
    "for part in chatbot.stream({\"question\": \"Which model providers offer Large Language Models?\"}):\n",
    "    print(part)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
